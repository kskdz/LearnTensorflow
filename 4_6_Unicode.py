# -*- coding: utf-8 -*-
"""
Created on Fri Nov 20 16:54:21 2020
    å¤„ç†è‡ªç„¶è¯­è¨€çš„æ¨¡å‹é€šå¸¸ä½¿ç”¨ä¸åŒçš„å­—ç¬¦é›†æ¥å¤„ç†ä¸åŒçš„è¯­è¨€ã€‚Unicode æ˜¯ä¸€ç§æ ‡å‡†çš„ç¼–ç ç³»ç»Ÿï¼Œç”¨äºè¡¨ç¤ºå‡ ä¹æ‰€æœ‰è¯­è¨€çš„å­—ç¬¦ã€‚æ¯ä¸ªå­—ç¬¦ä½¿ç”¨ 0 å’Œ 0x10FFFF ä¹‹é—´çš„å”¯ä¸€æ•´æ•°ç ä½è¿›è¡Œç¼–ç ã€‚Unicode å­—ç¬¦ä¸²æ˜¯ç”±é›¶ä¸ªæˆ–æ›´å¤šç ä½ç»„æˆçš„åºåˆ—ã€‚
    æœ¬æ•™ç¨‹ä»‹ç»äº†å¦‚ä½•åœ¨ TensorFlow ä¸­è¡¨ç¤º Unicode å­—ç¬¦ä¸²ï¼Œä»¥åŠå¦‚ä½•ä½¿ç”¨æ ‡å‡†å­—ç¬¦ä¸²è¿ç®—çš„ Unicode ç­‰æ•ˆé¡¹å¯¹å…¶è¿›è¡Œæ“ä½œã€‚å®ƒä¼šæ ¹æ®å­—ç¬¦ä½“ç³»æ£€æµ‹å°† Unicode å­—ç¬¦ä¸²åˆ’åˆ†ä¸ºä¸åŒè¯ä¾‹ã€‚
@author: 67443
"""

import tensorflow as tf

'æ‚¨å¯ä»¥ä½¿ç”¨åŸºæœ¬çš„ TensorFlow tf.string dtype æ„å»ºå­—èŠ‚å­—ç¬¦ä¸²å¼ é‡ã€‚Unicode å­—ç¬¦ä¸²é»˜è®¤ä½¿ç”¨ UTF-8 ç¼–ç ã€‚'
tf.constant(u"Thanks ğŸ˜Š")
'tf.string å¼ é‡å¯ä»¥å®¹çº³ä¸åŒé•¿åº¦çš„å­—èŠ‚å­—ç¬¦ä¸²ï¼Œå› ä¸ºå­—èŠ‚å­—ç¬¦ä¸²ä¼šè¢«è§†ä¸ºåŸå­å•å…ƒã€‚å­—ç¬¦ä¸²é•¿åº¦ä¸åŒ…æ‹¬åœ¨å¼ é‡ç»´åº¦ä¸­ã€‚'
tf.constant([u"You're", u"welcome!"]).shape
'åœ¨ v3 ä¸­ï¼Œå­—ç¬¦ä¸²é»˜è®¤ä½¿ç”¨ Unicode ç¼–ç ã€‚'

'Unicodeè¡¨ç¤º'
'ä¸‰ç§è¡¨ç¤ºæ–¹æ³•'
'''
ä¸¤ç§è¡¨ç¤ºæ–¹æ³•ï¼š
    string æ ‡é‡ - ä½¿ç”¨å·²çŸ¥å­—ç¬¦ç¼–ç å¯¹ç ä½åºåˆ—è¿›è¡Œç¼–ç ã€‚
    <tf.Tensor: shape=(), dtype=string, numpy=b'\xe8\xaf\xad\xe8\xa8\x80\xe5\xa4\x84\xe7\x90\x86'>
    int32 å‘é‡ - æ¯ä¸ªä½ç½®åŒ…å«å•ä¸ªç ä½ã€‚
    <tf.Tensor: shape=(4,), dtype=int32, numpy=array([35821, 35328, 22788, 29702], dtype=int32)>
'''
# Unicode string, represented as a UTF-8 encoded string scalar.
text_utf8 = tf.constant(u"è¯­è¨€å¤„ç†")  #åˆ›å»ºå‡½æ•°
text_utf8

# Unicode string, represented as a UTF-16-BE encoded string scalar.
text_utf16be = tf.constant(u"è¯­è¨€å¤„ç†".encode("UTF-16-BE"))
text_utf16be

# Unicode string, represented as a vector of Unicode code points.
text_chars = tf.constant([ord(char) for char in u"è¯­è¨€å¤„ç†"])
text_chars  #ord() è¿”å›å¯¹åº”çš„ ASCII æ•°å€¼ï¼Œæˆ–è€… Unicode æ•°å€¼


'ä¸åŒç±»å‹è½¬æ¢'
'å‘é‡æ ‡é‡è½¬åŒ–ï¼›ä¸åŒç¼–ç æ–¹å¼è½¬åŒ–'
'tf.strings.unicode_decodeï¼šå°†ç¼–ç çš„å­—ç¬¦ä¸²æ ‡é‡è½¬æ¢ä¸ºç ä½çš„å‘é‡ã€‚'
tf.strings.unicode_decode(text_utf8,
                          input_encoding='UTF-8')

'tf.strings.unicode_encodeï¼šå°†ç ä½çš„å‘é‡è½¬æ¢ä¸ºç¼–ç çš„å­—ç¬¦ä¸²æ ‡é‡ã€‚'
tf.strings.unicode_encode(text_chars,
                          output_encoding='UTF-8')

'tf.strings.unicode_transcodeï¼šå°†ç¼–ç çš„å­—ç¬¦ä¸²æ ‡é‡è½¬æ¢ä¸ºå…¶ä»–ç¼–ç ã€‚'
tf.strings.unicode_transcode(text_utf8,
                             input_encoding='UTF8',
                             output_encoding='UTF-16-BE')


'é•¿åº¦å¤„ç†é—®é¢˜ï¼šé•¿åº¦ä¸åŒçš„ç±»å‹è½¬åŒ–'
'æ‰¹æ¬¡ç»´åº¦ï¼šè§£ç å¤šä¸ªå­—ç¬¦ä¸²æ—¶ï¼Œæ¯ä¸ªå­—ç¬¦ä¸²ä¸­çš„å­—ç¬¦æ•°å¯èƒ½ä¸ç›¸ç­‰ã€‚è¿”å›ç»“æœæ˜¯ tf.RaggedTensor æ®‹å·®ä¸èµ·çš„äº§é‡'
# A batch of Unicode strings, each represented as a UTF8-encoded string.
batch_utf8 = [s.encode('UTF-8') for s in
              [u'hÃƒllo',  u'What is the weather tomorrow',  u'GÃ¶Ã¶dnight', u'ğŸ˜Š']]
batch_chars_ragged = tf.strings.unicode_decode(batch_utf8,
                                               input_encoding='UTF-8')
for sentence_chars in batch_chars_ragged.to_list():
  print(sentence_chars)
  
'ä¹Ÿå¯ä»¥ä½¿ç”¨ tf.RaggedTensor.to_tensor å’Œ tf.RaggedTensor.to_sparse æ–¹æ³•å°†å…¶è½¬æ¢ä¸ºå¸¦æœ‰å¡«å……çš„å¯†é›† tf.Tensor æˆ– tf.SparseTensor'
batch_chars_padded = batch_chars_ragged.to_tensor(default_value=-1)
print(batch_chars_padded.numpy())

batch_chars_sparse = batch_chars_ragged.to_sparse()
#ç¨€ç–è½¬åŒ–ä¸º[å¥å­ç¼–å· å­—ç¬¦å·]

#ç­‰é•¿å­—ç¬¦è½¬åŒ–
tf.strings.unicode_encode([[99, 97, 116], [100, 111, 103], [ 99, 111, 119]],
                          output_encoding='UTF-8')
#ä¸ç­‰é•¿å­—ç¬¦è½¬åŒ–
tf.strings.unicode_encode(batch_chars_ragged, output_encoding='UTF-8')

'å¦‚æœæ‚¨çš„å¼ é‡å…·æœ‰å¡«å……æˆ–ç¨€ç–æ ¼å¼çš„å¤šä¸ªå­—ç¬¦ä¸²ï¼Œè¯·åœ¨è°ƒç”¨ unicode_encode ä¹‹å‰å°†å…¶è½¬æ¢ä¸º tf.RaggedTensor'
tf.strings.unicode_encode(
    tf.RaggedTensor.from_sparse(batch_chars_sparse),
    output_encoding='UTF-8')

tf.strings.unicode_encode(
    tf.RaggedTensor.from_tensor(batch_chars_padded, padding=-1),
    output_encoding='UTF-8')

'Unicode è¿ç®—'
#é•¿åº¦è®¡ç®—
# Note that the final character takes up 4 bytes in UTF8.
thanks = u'Thanks ğŸ˜Š'.encode('UTF-8')
num_bytes = tf.strings.length(thanks).numpy()
num_chars = tf.strings.length(thanks, unit='UTF8_CHAR').numpy()
print('{} bytes; {} UTF-8 characters'.format(num_bytes, num_chars))

#å­å­—ç¬¦ä¸²
#tf.strings.substr è¿ç®—ä¼šæ¥å— "unit" å‚æ•°ï¼Œå¹¶ç”¨å®ƒæ¥ç¡®å®š "pos" å’Œ "len" å‚æ•°åŒ…å«çš„åç§»ç±»å‹
# default: unit='BYTE'. With len=1, we return a single byte.
tf.strings.substr(thanks, pos=7, len=1).numpy()
# Specifying unit='UTF8_CHAR', we return a single character, which in this case
# is 4 bytes.
print(tf.strings.substr(thanks, pos=7, len=1, unit='UTF8_CHAR').numpy())

#æ‹†åˆ†å­—ç¬¦ä¸²
#tf.strings.unicode_split è¿ç®—ä¼šå°† Unicode å­—ç¬¦ä¸²æ‹†åˆ†ä¸ºå•ä¸ªå­—ç¬¦çš„å­å­—ç¬¦ä¸²
tf.strings.unicode_split(thanks, 'UTF-8').numpy()

#å­—ç¬¦çš„å­—èŠ‚åç§»é‡
#ä¸ºäº†å°† tf.strings.unicode_decode ç”Ÿæˆçš„å­—ç¬¦å¼ é‡ä¸åŸå§‹å­—ç¬¦ä¸²å¯¹é½ï¼Œäº†è§£æ¯ä¸ªå­—ç¬¦å¼€å§‹ä½ç½®çš„åç§»é‡å¾ˆæœ‰ç”¨ã€‚
#æ–¹æ³• tf.strings.unicode_decode_with_offsets ä¸ unicode_decode ç±»ä¼¼ï¼Œä¸åŒçš„æ˜¯å®ƒä¼šè¿”å›åŒ…å«æ¯ä¸ªå­—ç¬¦èµ·å§‹åç§»é‡çš„ç¬¬äºŒå¼ é‡ã€‚
codepoints, offsets = tf.strings.unicode_decode_with_offsets(u"ğŸˆğŸ‰ğŸŠ", 'UTF-8')

for (codepoint, offset) in zip(codepoints.numpy(), offsets.numpy()): #zipæ•ˆæœæ˜¯æ‰“åŒ…ä¸ºå…ƒç»„
  print("At byte offset {}: codepoint {}".format(offset, codepoint))


'Unicode å­—ç¬¦ä½“ç³»'
'''
æ¯ä¸ª Unicode ç ä½éƒ½å±äºæŸä¸ªç ä½é›†åˆï¼Œè¿™äº›é›†åˆè¢«ç§°ä½œå­—ç¬¦ä½“ç³»ã€‚æŸä¸ªå­—ç¬¦çš„å­—ç¬¦ä½“ç³»æœ‰åŠ©äºç¡®å®šè¯¥å­—ç¬¦å¯èƒ½æ‰€å±çš„è¯­è¨€ã€‚
ä¾‹å¦‚ï¼Œå·²çŸ¥ 'Ğ‘' å±äºè¥¿é‡Œå°”å­—ç¬¦ä½“ç³»ï¼Œè¡¨æ˜åŒ…å«è¯¥å­—ç¬¦çš„ç°ä»£æ–‡æœ¬å¾ˆå¯èƒ½æ¥è‡ªæŸä¸ªæ–¯æ‹‰å¤«è¯­ç§ï¼ˆå¦‚ä¿„è¯­æˆ–ä¹Œå…‹å…°è¯­ï¼‰ã€‚

TensorFlow æä¾›äº† tf.strings.unicode_script è¿ç®—æ¥ç¡®å®šæŸä¸€ç»™å®šç ä½ä½¿ç”¨çš„æ˜¯å“ªä¸ªå­—ç¬¦ä½“ç³»ã€‚
å­—ç¬¦ä½“ç³»ä»£ç æ˜¯å¯¹åº”äºå›½é™… Unicode ç»„ä»¶ (ICU) UScriptCode å€¼çš„ int32 å€¼ã€‚
'''
uscript = tf.strings.unicode_script([33464, 1041])  # ['èŠ¸', 'Ğ‘']

print(uscript.numpy())  # [17, 8] == [USCRIPT_HAN, USCRIPT_CYRILLIC]

#tf.strings.unicode_script è¿ç®—è¿˜å¯ä»¥åº”ç”¨äºç ä½çš„å¤šç»´ tf.Tensor æˆ– tf.RaggedTensor
print(tf.strings.unicode_script(batch_chars_ragged))

'ç¤ºä¾‹ï¼šç®€å•åˆ†è¯'
'''
åˆ†è¯æ˜¯å°†æ–‡æœ¬æ‹†åˆ†ä¸ºç±»ä¼¼å•è¯çš„å•å…ƒçš„ä»»åŠ¡ã€‚å½“ä½¿ç”¨ç©ºæ ¼å­—ç¬¦åˆ†éš”å•è¯æ—¶ï¼Œè¿™é€šå¸¸å¾ˆå®¹æ˜“ï¼Œ
ä½†æ˜¯æŸäº›è¯­è¨€ï¼ˆå¦‚ä¸­æ–‡å’Œæ—¥è¯­ï¼‰ä¸ä½¿ç”¨ç©ºæ ¼ï¼Œè€ŒæŸäº›è¯­è¨€ï¼ˆå¦‚å¾·è¯­ï¼‰ä¸­å­˜åœ¨é•¿å¤åˆè¯ï¼Œå¿…é¡»è¿›è¡Œæ‹†åˆ†æ‰èƒ½åˆ†æå…¶å«ä¹‰ã€‚
åœ¨ç½‘é¡µæ–‡æœ¬ä¸­ï¼Œä¸åŒè¯­è¨€å’Œå­—ç¬¦ä½“ç³»å¸¸å¸¸æ··åˆåœ¨ä¸€èµ·ï¼Œä¾‹å¦‚â€œNYæ ªä¾¡â€ï¼ˆçº½çº¦è¯åˆ¸äº¤æ˜“æ‰€ï¼‰ã€‚

æˆ‘ä»¬å¯ä»¥åˆ©ç”¨å­—ç¬¦ä½“ç³»çš„å˜åŒ–è¿›è¡Œç²—ç•¥åˆ†è¯ï¼ˆä¸å®ç°ä»»ä½• ML æ¨¡å‹ï¼‰ï¼Œä»è€Œä¼°ç®—è¯è¾¹ç•Œã€‚
è¿™å¯¹ç±»ä¼¼ä¸Šé¢â€œNYæ ªä¾¡â€ç¤ºä¾‹çš„å­—ç¬¦ä¸²éƒ½æœ‰æ•ˆã€‚è¿™ç§æ–¹æ³•å¯¹å¤§å¤šæ•°ä½¿ç”¨ç©ºæ ¼çš„è¯­è¨€ä¹Ÿéƒ½æœ‰æ•ˆï¼Œ
å› ä¸ºå„ç§å­—ç¬¦ä½“ç³»ä¸­çš„ç©ºæ ¼å­—ç¬¦éƒ½å½’ç±»ä¸º USCRIPT_COMMONï¼Œè¿™æ˜¯ä¸€ç§ç‰¹æ®Šçš„å­—ç¬¦ä½“ç³»ä»£ç ï¼Œä¸åŒäºä»»ä½•å®é™…æ–‡æœ¬ã€‚
'''
# dtype: string; shape: [num_sentences]
#
# The sentences to process.  Edit this line to try out different inputs!
sentence_texts = [u'Hello, world.', u'ä¸–ç•Œã“ã‚“ã«ã¡ã¯']

#å…ˆç¼–ç å­—ç¬¦ï¼šå†æ‰¾æ¯ä¸ªå­—ç¬¦çš„æ ‡è¯†ä½“ç³»
# dtype: int32; shape: [num_sentences, (num_chars_per_sentence)]
#
# sentence_char_codepoint[i, j] is the codepoint for the j'th character in
# the i'th sentence.
sentence_char_codepoint = tf.strings.unicode_decode(sentence_texts, 'UTF-8')
print(sentence_char_codepoint)

# dtype: int32; shape: [num_sentences, (num_chars_per_sentence)]
#
# sentence_char_scripts[i, j] is the unicode script of the j'th character in
# the i'th sentence.
sentence_char_script = tf.strings.unicode_script(sentence_char_codepoint)
print(sentence_char_script)

#æ·»åŠ è¯è¾¹ç•Œçš„ä½ç½®
# dtype: bool; shape: [num_sentences, (num_chars_per_sentence)]
#
# sentence_char_starts_word[i, j] is True if the j'th character in the i'th
# sentence is the start of a word.
'''
å‡½æ•°è¯´æ˜ï¼š
    tf.concat æ‹¼æ¥å¼ é‡çš„å‡½æ•°tf.concat()
         t1 = [[1, 2, 3], [4, 5, 6]]
         t2 = [[7, 8, 9], [10, 11, 12]]
         tf.concat([t1, t2], axis=0)  # [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]
         tf.concat([t1, t2], axis=1)  # [[1, 2, 3, 7, 8, 9], [4, 5, 6, 10, 11, 12]]
         æŒ‰ç…§æœ€å°‘çš„ä¸€åˆ—é“¾æ¥
         
    tf.fill(dims, value, name=None)  åˆ›å»ºä¸€ä¸ªç»´åº¦ä¸ºdimsï¼Œå€¼ä¸ºvalueçš„tensorå¯¹è±¡ã€‚
        a = tf.fill([10], 0)         #[0 0 0 0 0 0 0 0 0 0]
        b = tf.fill([2, 3, 4], 5)    #[[[5 5 5 5]  [5 5 5 5]  [5 5 5 5]] [[5 5 5 5]  [5 5 5 5]  [5 5 5 5]]]

    tf.not_equal(x,y,name = None)    è¿”å› (x! = y) å…ƒç´ çš„çœŸå€¼.
    tf.not_equal(sentence_char_script[:, 1:], sentence_char_script[:, :-1])]
    è¿™ä¸ªå†™æ³•å¾ˆå‰å®³å•Šï¼Œåˆ—è¡¨ä¸­ç›¸é‚»ä¸¤ä¸ªå…ƒç´ æ˜¯å¦ç›¸ç­‰
'''
sentence_char_starts_word = tf.concat(
    [tf.fill([sentence_char_script.nrows(), 1], True),
     tf.not_equal(sentence_char_script[:, 1:], sentence_char_script[:, :-1])],
    axis=1)

# dtype: int64; shape: [num_words]
#
# word_starts[i] is the index of the character that starts the i'th word (in
# the flattened list of characters from all sentences).
'''
    'tf.squeeze' å°†åŸå§‹inputä¸­æ‰€æœ‰ç»´åº¦ä¸º1çš„é‚£äº›ç»´éƒ½åˆ æ‰ axisæŒ‡å®šç»´åº¦
    tf.where(sentence_char_starts_word.values) #array([[ 0], [ 5], [ 7], ... ,        [15]], dtype=int64)>
    tf.squeeze(tf.where(sentence_char_starts_word.values), axis=1)
    #<tf.Tensor: shape=(6,), dtype=int64, numpy=array([ 0,  5,  7, 12, 13, 15], dtype=int64)>
'''

word_starts = tf.squeeze(tf.where(sentence_char_starts_word.values), axis=1)
print(word_starts)

#ç„¶åæ„å»º RaggedTensor
# dtype: int32; shape: [num_words, (num_chars_per_word)]
#
# word_char_codepoint[i, j] is the codepoint for the j'th character in the
# i'th word.
'tf.RaggedTensor.from_row_startsæ–¹æ³•'
word_char_codepoint = tf.RaggedTensor.from_row_starts(
    values=sentence_char_codepoint.values,
    row_starts=word_starts)
print(word_char_codepoint)

#æœ€ååˆ’åˆ†åˆ°å¥å­ä¸­ è¯ç ä½ RaggedTensor åˆ’åˆ†å›å¥å­
# dtype: int64; shape: [num_sentences]
#
# sentence_num_words[i] is the number of words in the i'th sentence.
'tf.reduce_sum ç”¨äºè®¡ç®—å¼ é‡tensoræ²¿ç€æŸä¸€ç»´åº¦çš„å’Œï¼Œå¯ä»¥åœ¨æ±‚å’Œåé™ç»´'
sentence_num_words = tf.reduce_sum(
    tf.cast(sentence_char_starts_word, tf.int64),  #æ•°æ®ç±»å‹è½¬æ¢
    axis=1)

# dtype: int32; shape: [num_sentences, (num_words_per_sentence), (num_chars_per_word)]
#
# sentence_word_char_codepoint[i, j, k] is the codepoint for the k'th character
# in the j'th word in the i'th sentence.
'tf.RaggedTensor.from_row_lengths'
sentence_word_char_codepoint = tf.RaggedTensor.from_row_lengths(
    values=word_char_codepoint,
    row_lengths=sentence_num_words)
print(sentence_word_char_codepoint)

#è½¬åŒ–ä¸ºUTF-8
tf.strings.unicode_encode(sentence_word_char_codepoint, 'UTF-8').to_list()

